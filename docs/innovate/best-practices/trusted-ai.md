---
title: 負責任和受信任的 AI
description: 瞭解負責任 AI 的六個指導準則-責任、包容性、可靠性和安全性、公平、透明度，以及隱私權和安全性。
author: msteller-Ai
ms.author: brblanch
ms.date: 01/20/2021
ms.topic: conceptual
ms.service: cloud-adoption-framework
ms.subservice: innovate
ms.custom: think-tank
ms.openlocfilehash: 8fa6f164cea1a5382a2a6591052936cb18a76784
ms.sourcegitcommit: b8f8b7631aabaab28e9705934bf67dad15e3a179
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 03/03/2021
ms.locfileid: "101792749"
---
# <a name="responsible-and-trusted-ai"></a>負責任和受信任的 AI

Microsoft 概述了負責任 AI 的六個主要原則：責任、包容性、可靠性和安全性、公平、透明度，以及隱私權和安全性。 在將負責任且值得信賴的 AI 移至更主流的產品和服務時，這些原則是不可或缺的。 它們是由兩個觀點來引導：道德和解釋。

![負責任 AI 原則的圖表。](./media/responsible-ai-principles.png)

## <a name="ethical"></a>倫理

從道德的觀點來看，AI 在其判斷提示中應為公平和包容性，並負責其決策，而不會區分或阻礙不同的競爭、殘障或背景。

Microsoft 已在2017的工程和研究 [Aether](https://www.microsoft.com/ai/our-approach?activetab=pivot1%3aprimaryr5)中，建立 AI、道德和效果的道德委員會。 委員會的核心責任是建議負責任的問題、技術、流程和最佳作法。 若要深入瞭解 Aether，請參閱 [此 Microsoft 學習課程模組](/learn/modules/microsoft-responsible-ai-practices/3-microsoft-governance-model)。

### <a name="accountability"></a>責任

責任是負責任 AI 的重要要件。 設計和部署 AI 系統的人員必須負責其動作和決策，特別是當我們發展更多自主系統的過程。 組織應考慮建立內部審核主體，以提供有關開發和部署 AI 系統的監督、見解和指引。 雖然本指南可能會根據公司和地區而有所不同，但它應該會反映組織的 AI 旅程圖。

### <a name="inclusiveness"></a>包容性

包容性規定 AI 應考慮所有人為競爭和經驗，而包容性設計實務可協助開發人員瞭解並解決可能不慎排除人員的潛在阻礙。 可能的話，您應該使用語音轉換文字、文字轉換語音和視覺辨識技術，讓人們能夠有聽力、視覺和其他障礙。

### <a name="reliability-and-safety"></a>可靠性和安全性

AI 系統必須是可靠且安全的，才能受到信任。 系統必須以原本設計的方式來執行，並讓它安全地回應新的狀況。 其固有的復原能力應該會抵禦預期或非預期的操作。 應針對操作條件建立嚴格的測試和驗證，以確保系統能安全地回應邊緣案例，而 A/B 測試和冠軍/挑戰者方法則應整合到評估程式中。

AI 系統的效能可能會隨著時間而降低，因此必須建立穩固的監視和模型追蹤程式，以被動並主動測量模型的效能，並視需要重新定型，以將它現代化。

## <a name="what-is-explainable"></a>什麼是解釋

可解釋性可協助資料科學家、審計員和業務決策者確保 AI 系統可以合理地證明他們的決策，以及它們如何得出結論。 這也可確保符合公司原則、產業標準和政府法規的規範。 資料科學家應該能夠向專案關係人說明如何達到特定層級的精確度，以及影響此結果的原因。 同樣地，為了符合公司的原則，審計員需要可驗證模型的工具，而商務決策者必須能夠提供透明模型，才能獲得信任。

### <a name="explainability-tools"></a>可解釋性工具

Microsoft 開發了 [InterpretML](https://interpret.ml/)，這是一個開放原始碼工具組，可協助達成模型可解釋性並支援半透明方塊和黑色方塊模型。

- 玻璃方塊模型的結構是可解譯的。 針對這些模型，請使用解釋提升機器，也就是根據決策樹或線性模型的演算法狀態，提供不失真的說明，而且可供網域專家編輯。

- 由於有複雜的內部結構（類神經網路），因此會更難解讀黑色方塊模型。 Explainers 像是橙色或 Shapley additive explanations 加法狀說明 (SHAP) 藉由分析輸入和輸出之間的關聯性來解讀這些模型。

- [Fairlearn](https://fairlearn.org/) 是 Azure Machine Learning 整合，以及適用于 SDK 和 AutoML 圖形使用者介面的開放原始碼工具組。 使用 explainers 來瞭解主要會對模型和網域專家造成哪些影響，以驗證這些影響。

探索 [Azure Machine Learning 中的模型可解譯性](/azure/machine-learning/how-to-machine-learning-interpretability) ，以深入瞭解可解釋性。

### <a name="fairness"></a>公平性

公平是所有人類都要瞭解並套用的核心道德原則。 開發 AI 系統時，此原則更重要。 主要檢查和餘額需要確保系統的決策不會區分或執行對群組或個人的性別、競爭、色情方向或宗教偏差。

- Microsoft 提供的 [ai 公平檢查清單](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA) 提供 ai 系統的指引和解決方案。 這些解決方案鬆散分為五個階段：構想、原型、組建、啟動及發展。 每個階段都會列出建議的審慎活動，有助於將不公平性在系統中的影響降到最低。

- Fairlearn 與 Azure Machine Learning 整合，並支援資料科學家和開發人員來評定和改善其 AI 系統的公平。 工具箱提供各種不公平性防護演算法，以及視覺化模型公平的互動式儀表板。 使用工具組，並在建立模型時，仔細評估模型的公平;這應該是資料科學流程不可或缺的一部分。

瞭解如何 [減輕機器學習模型中的公平](/azure/machine-learning/concept-fairness-ml)。

### <a name="transparency"></a>透明度

達到透明度可協助小組瞭解用來定型模型的資料和演算法、套用至資料的轉換邏輯、產生的最終模型，以及其相關聯的資產。 這項資訊可提供模型建立方式的深入解析，讓它能夠以透明的方式重新複製。 [Azure Machine Learning 工作區](/azure/machine-learning/concept-workspace)中的快照集會藉由錄製或重新定型與實驗相關的所有定型相關資產和計量，來支援透明度。

### <a name="privacy-and-security"></a>隱私權和安全性

資料持有者有義務保護 AI 系統中的資料，而隱私權和安全性是此系統不可或缺的一部分。 個人必須受到保護，而且應以不危及個人隱私權的方式存取。 [Azure 差異隱私權](/azure/machine-learning/concept-differential-privacy) 藉由隨機化資料並新增干擾來隱藏資料科學家的個人資訊，來保護及保留隱私權。

## <a name="human-ai-guidelines"></a>人類 AI 指導方針

人類 AI 設計指導方針是由四個週期所組成：一開始在互動期間、發生錯誤時，以及經過一段時間。 這些原則的設計目的是要產生更包容和人為中心的 AI 系統。

### <a name="initially"></a>最初

- **澄清系統的功能。** 如果 AI 系統使用或產生計量，請務必將它們全部顯示出來，並加以追蹤。

- **明確說明系統可以做的事。** 協助使用者瞭解 AI 的精確度不會是 100-%，而且會設定 AI 系統可能發生錯誤的預期。

### <a name="during-interaction"></a>在互動期間

- **顯示內容的相關資訊。** 提供與使用者目前內容和環境相關的視覺資訊，例如鄰近飯店，並傳回接近目標目的地和日期的詳細資料。

- **減少社交偏差。** 請確定語言和行為不會引進非預期的造型或偏差。 例如，自動完成功能必須同時認可這兩個性別。

### <a name="when-wrong"></a>錯誤時

- **支援有效率的關閉。** 提供簡單的機制，以忽略或關閉非預期的功能/服務。
- **支援有效率的修正。** 提供直覺的方式，讓您更輕鬆地編輯、修改或復原。
- **請清楚瞭解系統執行它的原因。** 將解釋 AI 優化，以提供 AI 系統判斷提示的見解。

### <a name="over-time"></a>經過一段時間

- **記住最近的互動。** 保留互動歷程記錄以供日後參考。
- **瞭解使用者行為。** 根據使用者的行為將互動個人化。
- **請小心更新並調整。** 限制干擾性變更，並根據使用者的設定檔更新。
- **鼓勵細微的意見反應。** 收集使用者與 AI 系統互動的意見反應。

## <a name="a-persona-centric-trusted-ai-framework"></a>以角色為中心的受信任 AI 架構

![以角色為中心、受信任的 AI 架構圖表。](./media/ai-framework.png)

### <a name="ai-designer"></a>AI 設計工具

AI 設計工具會建立模型，並負責：

- 資料漂移和品質檢查。 他們會偵測極端值並執行資料品質檢查，以找出遺漏值、標準化散發、檢查資料，以及產生使用案例和專案報表。

- 評估系統來源中的資料，以識別潛在偏差。

- 設計 AI 演算法來將資料偏差降至最低，例如探索分類收納、群組和正規化 (特別是在傳統機器學習模型中（例如以樹狀結構為基礎的模型）) 可以從資料中排除少數群組。 類別 AI 設計 reiterates 資料偏差的方法，是在產業縱向中分組社交、種族和性別類別，其依賴受保護的健康資訊 (PHI) 和個人識別資訊 (PII) 。

- 優化監視和警示，以識別目標洩漏並強化模型的開發。

- 建立報告和深入解析的最佳作法，以提供更細微的模型瞭解，並避免使用特徵或向量重要性、UMAP 叢集、Friedman 的 H-統計資料、功能效果和其他的黑箱方法。 識別計量有助於定義複雜和新式資料集中相互關聯之間的預測性影響、關聯性和相依性。

### <a name="ai-administrators-and-officers"></a>AI 系統管理員和主管

AI 系統管理員和主管監督 AI、治理和 audit framework 作業和效能計量，以及 AI 安全性的實施方式，以及商務投資報酬率。

- 監視追蹤儀表板，以協助進行模型監視、結合生產模型的模型計量，並將焦點放在精確度、模型降級、資料漂移、偏差，以及加速/錯誤推斷的變更。

- 採用彈性的部署和重新部署 (最好是 REST API) 可讓模型實作為開放、中立的架構，將模型與商務程式整合，並產生意見反應迴圈的價值。

- 致力於建立模型治理和存取權，並可降低負面的商務和操作衝擊。 以角色為基礎的存取控制標準可決定安全性控制項，以保留受限制的生產環境和 IP。

- 使用 AI 審核和合規性架構來追蹤模型的開發和變更，以遵守產業特定標準。 可解譯和負責任的 AI 以可解釋性量值、精簡功能、模型視覺效果和產業垂直語言為依據。

### <a name="ai-business-consumer"></a>AI 商務消費者

AI 商務消費者 (商務專家) 關閉意見反應迴圈，並為 AI 設計工具提供輸入。 預測性決策和潛在偏差的含意，例如公平和道德措施、隱私權和合規性，以及商務效率有助於評估 AI 系統。

- 意見反應迴圈屬於企業的生態系統。 顯示模型偏差、錯誤、預測速度和公平的資料會在 AI 設計師、系統管理員和主管之間建立信任和平衡。 以人為中心的評量應能逐漸改進 AI，以及將多維度的 AI 學習最小化，而複雜的資料 (縮短的學習) 有助於防止偏差學習。

- 使用可解譯性的設計和工具，可讓 AI 系統對潛在偏差負負責任。 模型偏差和公平問題應該加上旗標並送至警示和異常偵測系統，此系統會從此行為學習並自動解決偏差。

- 每個預測值都應該依重要性或影響細分為個別特徵或向量，並提供可匯出至商務報表以進行審核和合規性審核、客戶透明度和企業就緒的完整預測說明。

- 由於有越來越多的安全性和隱私權風險，在推斷期間解決資料違規的最佳作法需要遵守個別產業的各項法規;例如，關於 PHI 和 PII 不符合規範的警示、違反全國安全性法律等等。

## <a name="next-steps"></a>下一步

探索 [人類 ai 指導方針](/ai/guidelines-human-ai-interaction/) ，以深入瞭解負責任的 ai。
